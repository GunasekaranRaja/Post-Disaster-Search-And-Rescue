{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive # creating a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "%cd /mydrive/FYP/Ensemble"
      ],
      "metadata": {
        "id": "gdzdYXF90TMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJjz2TfKy6JB"
      },
      "outputs": [],
      "source": [
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies\n",
        "!git clone https://github.com/ultralytics/yolov5.git  # clone repo\n",
        "%cd yolov5\n",
        "!python -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1d1A_fKtXIKW1zCf_Lyqj3Q-zrISLlVbZ', 'yolov5s.pt')\"  # download yolov5s.pt\n",
        "%cd ..\n",
        "!wget https://github.com/facebookresearch/detectron2/archive/refs/tags/v0.6.1.zip\n",
        "!unzip v0.6.1.zip\n",
        "%cd detectron2-0.6.1/\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.transforms = transforms\n",
        "        \n",
        "        # Load the image filepaths and annotations from a CSV file\n",
        "        self.data = pd.read_csv(os.path.join(data_dir, 'annotations.csv'))\n",
        "        self.img_dir = os.path.join(data_dir, 'images')\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image and its annotations\n",
        "        img_path = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        boxes = torch.tensor(eval(self.data.iloc[idx, 1]), dtype=torch.float32)\n",
        "        labels = torch.tensor(eval(self.data.iloc[idx, 2]), dtype=torch.int64)\n",
        "        \n",
        "        # Apply transforms\n",
        "        img, boxes, labels = self.transforms(img, boxes, labels)\n",
        "        \n",
        "        # Return a dictionary containing the image and its annotations\n",
        "        return {'image': img, 'boxes': boxes, 'labels': labels}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "KqaZ1Ft03LZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def calculate_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            images, targets = batch['image'].to(device), batch['targets']\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # Iterate over each image in the batch\n",
        "            for i in range(len(targets)):\n",
        "                # Get the ground truth and predicted boxes and labels for the current image\n",
        "                gt_boxes = targets[i]['boxes'].to(device)\n",
        "                gt_labels = targets[i]['labels'].to(device)\n",
        "                pred_boxes = outputs[i]['boxes']\n",
        "                pred_labels = outputs[i]['labels']\n",
        "                \n",
        "                # Calculate the number of correctly detected objects\n",
        "                correct = 0\n",
        "                for j in range(len(gt_labels)):\n",
        "                    if gt_labels[j] in pred_labels and \\\n",
        "                       (pred_boxes[pred_labels == gt_labels[j]] == gt_boxes[j]).any():\n",
        "                        correct += 1\n",
        "                total_correct += correct\n",
        "                total_images += 1\n",
        "    \n",
        "    accuracy = total_correct / total_images\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "-FenZ_op5jPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imShow(path):\n",
        "    import cv2\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "    image = cv2.imread(path)\n",
        "    height, width = image.shape[:2]\n",
        "    resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(18, 10)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nysDirMNHeY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from yolov5.models import YOLOv5\n",
        "\n",
        "# Define the YOLOv5 model\n",
        "model_yolov5 = YOLOv5(num_classes=num_classes)\n",
        "\n",
        "# Define the optimizer and the loss function\n",
        "optimizer = torch.optim.Adam(model_yolov5.parameters(), lr=lr)\n",
        "loss_fn = YOLOv5Loss(num_classes=num_classes, strides=model_yolov5.strides)\n",
        "\n",
        "# Define the data loader\n",
        "dataset = CustomDataset(annotations_file)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Train the YOLOv5 model\n",
        "for epoch in range(num_epochs):\n",
        "    for images, targets in data_loader:\n",
        "        images = images.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model_yolov5(images)\n",
        "\n",
        "        loss = sum([loss_fn(outputs[i], targets[i]) for i in range(len(outputs))])\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "yolov5_accuracy = calculate_accuracy(model_yolov5, data_loader, device)\n",
        "print(f'YOLOv5 Accuracy: {yolov5_accuracy}')\n",
        "imShow('chart.png')"
      ],
      "metadata": {
        "id": "ard34thE3pvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, launch\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data import build_detection_train_loader\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "# Define the dataset directory and the annotations file\n",
        "dataset_dir = 'path/to/dataset'\n",
        "annotations_file = 'path/to/annotations.json'\n",
        "\n",
        "# Register the custom dataset\n",
        "register_coco_instances('custom_dataset_train', {}, annotations_file, dataset_dir)\n",
        "\n",
        "# Define the configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/faster_rcnn_R_50_C4_3x.yaml'))\n",
        "cfg.DATASETS.TRAIN = ('custom_dataset_train',)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.DEVICE = 'cuda'\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-Detection/faster_rcnn_R_50_C4_3x.yaml')\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.02\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
        "\n",
        "# Train the Faster R-CNN model\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "imShow('chart.png')\n",
        "# faster_rcnn_accuracy = calculate_accuracy(model_yolov5, cfg.DATALOADER, cfg.MODEL.DEVICE)\n",
        "# print(f'Faster RCNN Accuracy: {faster_rcnn_accuracy}')\n"
      ],
      "metadata": {
        "id": "qIBFfhlN3vkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# Load the test image\n",
        "image = Image.open('path/to/test/image.jpg')\n",
        "\n",
        "# Use the Faster R-CNN model to detect objects\n",
        "cfg.MODEL.WEIGHTS = 'path/to/faster_rcnn/model.pth'\n",
        "predictor = DefaultPredictor(cfg)\n",
        "output_frcnn = predictor(image)\n",
        "\n",
        "# Use the YOLOv5 model to detect objects\n",
        "model_yolov5.to(device)\n",
        "model_yolov5.eval()\n",
        "with torch.no_grad():\n",
        "    image_tensor = transforms(image).unsqueeze(0).to(device)\n",
        "    output_yolov5 = model_yolov5(image_tensor)\n",
        "    output_yolov5 = non_max_suppression(output_yolov5, conf_thres=0.5, iou_thres=0.5)\n",
        "\n",
        "# Visualize the results from Faster R-CNN\n",
        "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "v = Visualizer(np.array(image), metadata=metadata, scale=1.0)\n",
        "v = v.draw_instance_predictions(output_frcnn['instances'].to('cpu'))\n",
        "Image.fromarray(v.get_image())\n",
        "\n",
        "# Visualize the results from YOLOv5\n",
        "image_yolov5 = plot_one_box(output_yolov5[0][0][:4].cpu().numpy(), np.array(image), color=[0,0,255], line_thickness=2)\n",
        "Image.fromarray(image_yolov5)\n"
      ],
      "metadata": {
        "id": "O4PYjEIA31p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = [output_frcnn, output_yolov5]\n",
        "result_ensemble = ensemble_results(results)\n"
      ],
      "metadata": {
        "id": "xRLXo_6U3327"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def plot_results(image, result):\n",
        "#     fig, ax = plt.subplots(figsize=(10, 10))\n",
        "#     ax.imshow(image)\n",
        "\n",
        "#     for box in result.xyxy[0]:\n",
        "#         x1, y1, x2, y2 = box[:4].tolist()\n",
        "#         score = box[4].item()\n",
        "#         width = max(2, int(image.width / 400))\n",
        "#         ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', lw=width\n"
      ],
      "metadata": {
        "id": "rfxeiIj534ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(image, instances):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(image)\n",
        "    colors = plt.cm.hsv(np.linspace(0, 1, len(instances)+1)).tolist()\n",
        "\n",
        "    # Plot each instance with a unique color\n",
        "    for i, inst in enumerate(instances):\n",
        "        color = colors[i]\n",
        "        bbox = inst['bbox']\n",
        "        label = inst['label']\n",
        "        score = inst['score']\n",
        "        ax.add_patch(plt.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
        "                                    fill=False, edgecolor=color, linewidth=2))\n",
        "        ax.text(bbox[0], bbox[1] - 2, f'{label} {score:.2f}', fontsize=12,\n",
        "                color=color, ha='center', va='top')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sdzAucAZ5AH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the ensemble results\n",
        "plot_results(image, result_ensemble)"
      ],
      "metadata": {
        "id": "5x-QpgTY4Ztk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------old----------------------------------------"
      ],
      "metadata": {
        "id": "3BpF7AMC3qjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# model_yolov5 = torch.hub.load('ultralytics/yolov5', 'custom', path_or_model='yolov5s.pt')"
      ],
      "metadata": {
        "id": "DQYFs-3FzFYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# from detectron2.config import get_cfg\n",
        "# from detectron2.engine import DefaultPredictor\n",
        "\n",
        "# # Create the configuration for the Faster R-CNN model\n",
        "# cfg = get_cfg()\n",
        "# cfg.merge_from_file(\"detectron2-0.6.1/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "\n",
        "# # Download the weights for the Faster R-CNN model\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2-0.6.1/model_final_faster_rcnn.pth\"\n",
        "\n",
        "# # Create the predictor for the Faster R-CNN model\n",
        "# model_frcnn = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "tEoCLjWWzHl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "\n",
        "# image_path = 'path/to/image.jpg'\n",
        "# image = Image.open(image_path)\n",
        "\n",
        "# result_yolov5 = model_yolov5(image)\n",
        "# result_frcnn = model_frcnn(cv2.imread(image_path)[:, :, ::-1])"
      ],
      "metadata": {
        "id": "IhjWtbdAzJU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def ensemble_results(results):\n",
        "#     # Combine the bounding box coordinates and confidence scores from each model\n",
        "#     boxes = []\n",
        "#     scores = []\n",
        "#     for result in results:\n",
        "#         boxes.append(result.xyxy[0])\n",
        "#         scores.append(result.xyxy[0][:, 4])\n",
        "#     boxes = torch.stack(boxes)\n",
        "#     scores = torch.stack(scores)\n",
        "\n",
        "#     # Compute the weighted average of the bounding box coordinates and confidence scores\n",
        "#     weights = scores / torch.sum(scores, dim=0, keepdim=True)\n",
        "#     weighted_boxes = torch.sum(weights.unsqueeze(-1) * boxes, dim=0)\n",
        "#     weighted_scores = torch.mean(scores, dim=0)\n",
        "\n",
        "#     # Create a new result object with the ensemble results\n",
        "#     result = results[0].clone()\n",
        "#     result.xyxy[0] = weighted_boxes\n",
        "#     result.pandas().xyxy[0]['confidence'] = weighted_scores.cpu().numpy()"
      ],
      "metadata": {
        "id": "f5X8_ZrEzLDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results = [result_yolov5, result_frcnn]\n",
        "# result_ensemble = ensemble_results(results)"
      ],
      "metadata": {
        "id": "iP8z_rZLzXLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def plot_results(image, result):\n",
        "#     fig, ax = plt.subplots(figsize=(10, 10))\n",
        "#     ax.imshow(image)\n",
        "\n",
        "#     for box in result.xyxy[0]:\n",
        "#         x1, y1, x2, y2 = box[:4].tolist()\n",
        "#         score = box[4].item()\n",
        "#         width = max(2, int(image.width / 400))\n",
        "#         ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', lw=width))\n",
        "#         text = f'{score:.2f}'\n",
        "#         ax.text(x1, y1, text, fontsize=max(14, width * 2), color='white', ha='left', va='top',\n",
        "#                 bbox=dict(facecolor='red', alpha=0.8, lw=0))\n",
        "#     plt.axis('off')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "vIYBAcFZzX6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_results(image, result_ensemble)"
      ],
      "metadata": {
        "id": "17Ths_drzbBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}